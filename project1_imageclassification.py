# -*- coding: utf-8 -*-
"""Project1_ImageClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-jQBtLaGQRpoY5MxwWMoqfrUvwsSBjyL

# Data Import and Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/ANNDL

"""## Importing Libraries"""

# Fix randomness and hide warnings
seed = 42

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=Warning)

import numpy as np
np.random.seed(seed)

import logging

import random
random.seed(seed)

# Import tensorflow
import tensorflow as tf
from tensorflow import keras as tfk
from tensorflow.keras import layers as tfkl
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel(logging.ERROR)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)
print(tf.__version__)
# Import other libraries
import cv2
from tensorflow.keras.applications.mobilenet import preprocess_input
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
import seaborn as sns
import pandas as pd

"""## Loading Data"""

public_data = np.load('public_data.npz', allow_pickle=True)
data = public_data['data']/255
labels = public_data['labels']

print(data.shape)
print(labels.shape)

"""We have 5200 samples of 96x96 images encoded in RGB."""

categories = np.unique(labels)
print(categories)

"""The images are divided into 2 classes : healthy and unhealthy"""

c_h = 0
c_u = 0
for label in labels:
  if label=='healthy':
    c_h += 1
  elif label=='unhealthy':
    c_u += 1

print(c_h)
print(c_u)

"""The healthy class has 3199 elements, the unhealthy class has 2001. Let's create a dictionary that we will use later in training our models with class imbalance."""

class_weights = [c_h/data.shape[0], c_u/data.shape[0]]
#print(class_weights)
class_weights = {0: class_weights[0], 1: class_weights[1]}
print(class_weights)

"""## Inspect Data and Data Cleaning"""

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(data[i])
  titolo = labels[i]
  ax.set_title(titolo)

"""From the first 10 images we can already understand that we can't distinguish healthy and unhealthy plants with the human eye. We print the whole dataset to be sure that there are no outliers. (Code is commented since it takes a long time to run)

batch_size = 100  # Adjust as needed
for i in range(0, len(data), batch_size):
    batch = data[i:i + batch_size]

    # Display images in the current batch
    for j, image in enumerate(batch):
        plt.subplot(10, 10, j + 1)  # Adjust subplot parameters as needed
        plt.imshow(image)
        #titolo = labels[i]
        plt.axis('off')
        plt.title(str(i + j))

    plt.show()

We notice some outliers in the form of shrek and trollolo pictures. Now we find them and remove them from our dataset

Finding Shreks
"""

first_shrek = data[95]
shreks = list()
shreks_labels = list()
shreks_indices = list()

for i in range(0,data.shape[0]):
  if (data[i] == first_shrek).all():
    shreks.append(data[i])
    shreks_labels.append(labels[i])
    shreks_indices.append(i)

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(shreks[i])
  ax.set_title(shreks_labels[i])

"""Trollolos"""

first_tro = data[338]
tro = list()
tro_labels = list()
tro_indices = list()

for i in range(0,data.shape[0]):
  if (data[i] == first_tro).all():
    tro.append(data[i])
    tro_labels.append(labels[i])
    tro_indices.append(i)

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(tro[i])
  ax.set_title(tro_labels[i])

"""Remove shreks and trollolos from the dataset"""

bad_data_indices = shreks_indices + tro_indices
data = np.delete(data, bad_data_indices, axis = 0)
labels = np.delete(labels, bad_data_indices)
data.shape

healthy = data[labels=='healthy']
unhealthy = data[labels=='unhealthy']

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(healthy[i])
  ax.set_title('healthy')

fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(unhealthy[i])
  ax.set_title('unhealthy')

"""We save the cleaned dataset in npz format and recompute the class weights."""

np.savez('cleaned_public_data.npz', array1=data, array2=labels)

c_h = 0
c_u = 0
for label in labels:
  if label=='healthy':
    c_h += 1
  elif label=='unhealthy':
    c_u += 1

print(c_h)
print(c_u)

class_weights = [c_h/data.shape[0], c_u/data.shape[0]]
#print(class_weights)
class_weights = {0: class_weights[0], 1: class_weights[1]}
print(class_weights)

"""## Train-Test Split

Now we use one-hot encoding to our categorical labels to be able to feed them to the network.
"""

# Convert to a pandas Series
labels_series = pd.Series(labels)

# Convert to categorical variable
categorical_labels = labels_series.astype('category')

# Convert to category codes
category_codes = categorical_labels.cat.codes

# Use tfk.utils.to_categorical to one-hot encode the category codes
y = tfk.utils.to_categorical(category_codes, num_classes=len(categorical_labels.cat.categories))

# The result is a one-hot encoded representation of your labels
print("One-Hot Encoded Labels:")
print(y)

"""And we split our dataset into training, validation and test set."""

X = data

# Split data into train_val and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=.25, stratify=np.argmax(y,axis=1))

# Further split train_val into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=np.argmax(y_train_val,axis=1))

# Print shapes of the datasets
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

"""Ok, now we have a training set, a validation set and a test set.

## Set Hyperparameters

Here we set the general hyperparameters that we will use for all our networks.
"""

# Define input shape, output shape, batch size, and number of epochs
input_shape = X_train.shape[1:]
output_shape_not_expanded = y_train.shape[1:]
output_shape = np.expand_dims(output_shape_not_expanded, axis=-1)
batch_size = 32
epochs = 1000

# Print input shape, batch size, and number of epochs
print(f"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}")

learning_rate = 1e-2

"""# Built-from-scratch Architectures

First we want to build our model and see if it's able to overfit the data, since we want to see if it can effectively learn the problem.

## LeNet

The first model that we build is LeNet
"""

def build_LeNet(input_shape=input_shape, output_shape=output_shape, seed=seed):
    tf.random.set_seed(seed)

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    conv1 = tfkl.Conv2D(
        filters=6,
        kernel_size=(5,5),
        padding='same',
        activation='tanh',
        name='conv1'
    )(input_layer)

    pool1 = tfkl.MaxPooling2D(
        pool_size=(2,2),
        name='mp1'
    )(conv1)

    conv2 = tfkl.Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='valid',
        activation='tanh',
        name='conv2'
    )(pool1)

    pool2 = tfkl.MaxPooling2D(
        pool_size =(2,2),
        name='mp2'
    )(conv2)

    flattening_layer=tfkl.Flatten(
        name='flatten'
    )(pool2)

    classifier_layer=tfkl.Dense(
        units=120,
        activation='tanh',
        name='dense1'
    )(flattening_layer)

    classifier_layer = tfkl.Dense(
        units=84,
        activation='tanh',
        name='dense2'
    )(classifier_layer)

    output_layer = tfkl.Dense(
        units=output_shape,
        activation='softmax',
        name='Output'
    )(classifier_layer)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='LeNet')

    # Compile the model
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])
    # model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy']) # AdamW applies the L2-norm. Extra stuff might be rewarded in the competition.

    # Return the model
    return model

model = build_LeNet(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

"""5 epochs since the model doesn't learn and we haven√¨t set Early stopping."""

epochs = 5

# Train the model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val)
).history

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

"""We can see that the original LeNet model cannot learn our problem, since just after epoch 30 the accuracy stagnates around 61.5%.

## Voluntary overfitting of quasiVGG9 model

We must improve our model. We have two alternatives: add more layers to LeNet or use VGG. ragazzi qui dobbiamo decidere se usare lenet con pi√π layers. io vado avanti con VGG.
"""

def build_quasiVGG9(input_shape, output_shape, seed=seed):
    tf.random.set_seed(seed)

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv00')(input_layer)
    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv01')(x)
    x = tfkl.MaxPooling2D(name='mp0')(x)

    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv10')(x)
    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv11')(x)
    x = tfkl.MaxPooling2D(name='mp1')(x)

    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv20')(x)
    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv21')(x)
    x = tfkl.MaxPooling2D(name='mp2')(x)

    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv30')(x)
    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv31')(x)
    x = tfkl.GlobalAveragePooling2D(name='gap')(x)

    output_layer = tfkl.Dense(units=output_shape,activation='softmax',name='Output')(x)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='Convnet')

    # Compile the model
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(weight_decay=5e-4), metrics=['accuracy'])

    # Return the model
    return model

model = build_quasiVGG9(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

epochs = 100

# Train the model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val)
).history

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

"""Still seems that it isn't learning.

## VGG16
"""

def build_VGG16(input_shape, output_shape, seed=seed):
    tf.random.set_seed(seed)

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv00')(input_layer)
    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv01')(x)
    x = tfkl.MaxPooling2D(name='mp0')(x)

    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv10')(x)
    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv11')(x)
    x = tfkl.MaxPooling2D(name='mp1')(x)

    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv20')(x)
    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv21')(x)
    x = tfkl.MaxPooling2D(name='mp2')(x)

    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv30')(x)
    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv31')(x)
    x = tfkl.MaxPooling2D(name='mp3')(x)

    x = tfkl.Conv2D(filters=512,kernel_size=3,padding='same',activation='relu',name='conv40')(x)
    x = tfkl.Conv2D(filters=512,kernel_size=3,padding='same',activation='relu',name='conv41')(x)
    x = tfkl.GlobalAveragePooling2D(name='gap')(x)

    output_layer = tfkl.Dense(units=output_shape,activation='softmax',name='Output')(x)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='Convnet')

    # Compile the model
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(weight_decay=5e-4), metrics=['accuracy'])

    # Return the model
    return model

model = build_VGG16(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

epochs = 30

# Train the model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val)
).history

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

"""## LeNet Augmentation"""

def build_LeNet_augmented(input_shape=input_shape, output_shape=output_shape, seed=seed):
    tf.random.set_seed(seed)

    preprocessing = tf.keras.Sequential([
        tfkl.RandomBrightness(0.2, value_range=(0,1)),
        tfkl.RandomTranslation(0.2,0.2),
    ], name='preprocessing')

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    preprocessing = preprocessing(input_layer)

    conv1 = tfkl.Conv2D(
        filters=6,
        kernel_size=(5,5),
        padding='same',
        activation='tanh',
        name='conv1'
    )(preprocessing)

    pool1 = tfkl.MaxPooling2D(
        pool_size=(2,2),
        name='mp1'
    )(conv1)

    conv2 = tfkl.Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='valid',
        activation='tanh',
        name='conv2'
    )(pool1)

    pool2 = tfkl.MaxPooling2D(
        pool_size =(2,2),
        name='mp2'
    )(conv2)

    flattening_layer=tfkl.Flatten(
        name='flatten'
    )(pool2)

    classifier_layer=tfkl.Dense(
        units=120,
        activation='tanh',
        name='dense1'
    )(flattening_layer)

    classifier_layer = tfkl.Dense(
        units=84,
        activation='tanh',
        name='dense2'
    )(classifier_layer)

    output_layer = tfkl.Dense(
        units=output_shape,
        activation='softmax',
        name='Output'
    )(classifier_layer)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='LeNet')

    # Compile the model
    # model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate), metrics=['accuracy'])
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy']) # AdamW applies the L2-norm. Extra stuff might be rewarded in the competition.

    # Return the model
    return model

def build_LeNet_augmented(input_shape=input_shape, output_shape=output_shape, seed=seed):
    tf.random.set_seed(seed)

    preprocessing = tf.keras.Sequential([
        tfkl.RandomBrightness(0.2, value_range=(0,1)),
        tfkl.RandomTranslation(0.2,0.2),
    ], name='preprocessing')

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    preprocessing = preprocessing(input_layer)

    conv1 = tfkl.Conv2D(
        filters=6,
        kernel_size=(5,5),
        padding='same',
        activation='tanh',
        name='conv1'
    )(preprocessing)

    pool1 = tfkl.MaxPooling2D(
        pool_size=(2,2),
        name='mp1'
    )(conv1)

    conv2 = tfkl.Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='valid',
        activation='tanh',
        name='conv2'
    )(pool1)

    pool2 = tfkl.MaxPooling2D(
        pool_size =(2,2),
        name='mp2'
    )(conv2)

    conv3 = tfkl.Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='valid',
        activation='tanh',
        name='conv3'
    )(pool2)

    pool3 = tfkl.MaxPooling2D(
        pool_size =(2,2),
        name='mp3'
    )(conv3)

    flattening_layer=tfkl.Flatten(
        name='flatten'
    )(pool3)

    classifier_layer=tfkl.Dense(
        units=120,
        activation='tanh',
        name='dense1'
    )(flattening_layer)

    classifier_layer = tfkl.Dense(
        units=84,
        activation='tanh',
        name='dense2'
    )(classifier_layer)

    output_layer = tfkl.Dense(
        units=output_shape,
        activation='softmax',
        name='Output'
    )(classifier_layer)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='LeNet')

    # Compile the model
    # model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate), metrics=['accuracy'])
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy']) # AdamW applies the L2-norm. Extra stuff might be rewarded in the competition.

    # Return the model
    return model

model = build_LeNet_augmented(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

epochs = 1000

callbacks = [
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True, mode='auto'),
]

# Train the model
aug_history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val),
    callbacks = callbacks
).history

model.save('LeNet_deeper_Aug')

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(aug_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(aug_history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(aug_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(aug_history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = model.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""Final Training on whole dataset with hyperparameters already tuned.

NB. automatizzare il calcolo del numero di epochs necessarie. o trovare optimal epoch dal codice prima.
"""

optimal_n_epochs = 366 - 100

# Build the model
model = build_LeNet_augmented(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

# Train the model
aug_history = model.fit(
    x = X,
    y = y,
    batch_size = batch_size,
    epochs = optimal_n_epochs,
    callbacks = callbacks
).history

model.save('LeNet_deeper_Aug_whole')

"""## LeNet Augmentation and more layers"""

def build_LeNet_augmented(input_shape=input_shape, output_shape=output_shape, seed=seed):
    tf.random.set_seed(seed)

    preprocessing = tf.keras.Sequential([
        tfkl.RandomBrightness(0.2, value_range=(0,1)),
        tfkl.RandomTranslation(0.2,0.2),
    ], name='preprocessing')

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    preprocessing = preprocessing(input_layer)

    conv1 = tfkl.Conv2D(
        filters=6,
        kernel_size=(5,5),
        padding='same',
        activation='tanh',
        name='conv1'
    )(preprocessing)

    pool1 = tfkl.MaxPooling2D(
        pool_size=(2,2),
        name='mp1'
    )(conv1)

    conv2 = tfkl.Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='valid',
        activation='tanh',
        name='conv2'
    )(pool1)

    pool2 = tfkl.MaxPooling2D(
        pool_size =(2,2),
        name='mp2'
    )(conv2)

    conv3 = tfkl.Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='valid',
        activation='tanh',
        name='conv3'
    )(pool2)

    pool3 = tfkl.MaxPooling2D(
        pool_size =(2,2),
        name='mp3'
    )(conv3)

    conv4 = tfkl.Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='valid',
        activation='tanh',
        name='conv4'
    )(pool3)

    pool4 = tfkl.MaxPooling2D(
        pool_size =(2,2),
        name='mp4'
    )(conv4)

    flattening_layer=tfkl.Flatten(
        name='flatten'
    )(pool4)

    classifier_layer=tfkl.Dense(
        units=120,
        activation='tanh',
        name='dense1'
    )(flattening_layer)

    classifier_layer = tfkl.Dense(
        units=84,
        activation='tanh',
        name='dense2'
    )(classifier_layer)

    output_layer = tfkl.Dense(
        units=output_shape,
        activation='softmax',
        name='Output'
    )(classifier_layer)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='LeNet')

    # Compile the model
    # model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate), metrics=['accuracy'])
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy']) # AdamW applies the L2-norm. Extra stuff might be rewarded in the competition.

    # Return the model
    return model

epochs = 1000

# Build the model
model = build_LeNet_augmented(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

# callbacks setting.
callbacks = [
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True, mode='auto'),
]
# Train the model
aug_history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val),
    callbacks = callbacks
).history

model.save('LeNet_even_deeper_Aug')

tf.keras.models.save_model(model, '/gdrive/My Drive/ANNDL')

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(aug_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(aug_history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(aug_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(aug_history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = model.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## LeNet+deeper+Aug+double conv

Is this VGG9?? NB maybe number of filters is too large. let's see how it goes.
"""

def build_LeNet_d_conv(input_shape=input_shape, output_shape=output_shape, seed=seed):
    tf.random.set_seed(seed)

    preprocessing = tf.keras.Sequential([
        tfkl.RandomBrightness(0.2, value_range=(0,1)),
        tfkl.RandomTranslation(0.2,0.2),
    ], name='preprocessing')

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    preprocessing = preprocessing(input_layer)

    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='tanh',name='conv00')(preprocessing)
    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='tanh',name='conv01')(x)
    x = tfkl.MaxPooling2D(name='mp0')(x)

    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='tanh',name='conv10')(x)
    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='tanh',name='conv11')(x)
    x = tfkl.MaxPooling2D(name='mp1')(x)

    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='tanh',name='conv20')(x)
    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='tanh',name='conv21')(x)
    x = tfkl.MaxPooling2D(name='mp2')(x)

    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='tanh',name='conv30')(x)
    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='tanh',name='conv31')(x)
    x = tfkl.GlobalAveragePooling2D(name='gap')(x)

    x=tfkl.Flatten(name='flatten')(x)

    x = tfkl.Dense(units=120,activation='tanh',name='dense1')(x)
    x = tfkl.Dense(units=84,activation='tanh',name='dense2')(x)

    output_layer = tfkl.Dense(units=output_shape,activation='softmax',name='Output')(x)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='LeNet')

    # Compile the model
    # model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate), metrics=['accuracy'])
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy']) # AdamW applies the L2-norm. Extra stuff might be rewarded in the competition.

    # Return the model
    return model

epochs = 1000

# Build the model
model = build_LeNet_d_conv(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

# callbacks setting.
callbacks = [
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True, mode='auto'),
]
# Train the model
aug_history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val),
    callbacks = callbacks
).history

model.save('LeNet_even_deeper_d_conv')

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(aug_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(aug_history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(aug_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(aug_history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = model.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## VGG9 Augmentation

I noticed that VGG9 wasn't good. I added 2 more dense layers and now i train. This is no more the VGG9 model.
"""

def build_quasiVGG9_augmented(input_shape, output_shape, seed=seed):
    tf.random.set_seed(seed)

    preprocessing = tf.keras.Sequential([
        tfkl.RandomBrightness(0.2, value_range=(0,1)),
        tfkl.RandomTranslation(0.2,0.2),
    ], name='preprocessing')

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    preprocessing = preprocessing(input_layer)

    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv00')(preprocessing)
    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv01')(x)
    x = tfkl.MaxPooling2D(name='mp0')(x)

    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv10')(x)
    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv11')(x)
    x = tfkl.MaxPooling2D(name='mp1')(x)

    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv20')(x)
    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv21')(x)
    x = tfkl.MaxPooling2D(name='mp2')(x)

    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv30')(x)
    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv31')(x)
    x = tfkl.GlobalAveragePooling2D(name='gap')(x)

    x = tfkl.Dense(units=120,activation='tanh',name='dense1')(x)
    x = tfkl.Dense(units=84,activation='tanh',name='dense2')(x)

    output_layer = tfkl.Dense(units=output_shape,activation='softmax',name='Output')(x)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='Convnet')

    # Compile the model
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(weight_decay=5e-4), metrics=['accuracy'])

    # Return the model
    return model

model = build_quasiVGG9_augmented(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

epochs = 5

# Train the model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val)
).history

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='quasiVGG9 Augmented', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='quasiVGG9 Augmented', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

"""### Set the early stopping."""

callbacks = [
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True, mode='auto'),
]

# Train the model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val),
    callbacks = callbacks
).history

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()



"""## VGG16 Augmentation

VGG16 with both preprocessing and more dense layers.
"""

def build_VGG16_augmented(input_shape, output_shape, seed=seed):
    tf.random.set_seed(seed)

    preprocessing = tf.keras.Sequential([
        tfkl.RandomBrightness(0.2, value_range=(0,1)),
        tfkl.RandomTranslation(0.2,0.2),
    ], name='preprocessing')

    # Build the neural network layer by layer
    input_layer = tfkl.Input(shape=input_shape, name='Input')

    preprocessing = preprocessing(input_layer)

    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv00')(preprocessing)
    x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv01')(x)
    x = tfkl.MaxPooling2D(name='mp0')(x)

    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv10')(x)
    x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv11')(x)
    x = tfkl.MaxPooling2D(name='mp1')(x)

    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv20')(x)
    x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv21')(x)
    x = tfkl.MaxPooling2D(name='mp2')(x)

    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv30')(x)
    x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv31')(x)
    x = tfkl.MaxPooling2D(name='mp3')(x)

    x = tfkl.Conv2D(filters=512,kernel_size=3,padding='same',activation='relu',name='conv40')(x)
    x = tfkl.Conv2D(filters=512,kernel_size=3,padding='same',activation='relu',name='conv41')(x)
    x = tfkl.GlobalAveragePooling2D(name='gap')(x)

    x = tfkl.Dense(units=120,activation='tanh',name='dense1')(x)
    x = tfkl.Dense(units=84,activation='tanh',name='dense2')(x)

    output_layer = tfkl.Dense(units=output_shape,activation='softmax',name='Output')(x)

    # Connect input and output through the Model class
    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='Convnet')

    # Compile the model
    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(weight_decay=5e-4), metrics=['accuracy'])

    # Return the model
    return model

model = build_VGG16_augmented(input_shape, output_shape)
model.summary()
tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)

epochs = 5

# Train the model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (X_val, y_val)
).history

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

"""## VGG like architecture with Batch Normalization"""

# Define a convolutional block with batch normalization option
def conv_bn_block(x, filters, kernel_size, padding='same', downsample=True, activation='relu', stack=2, batch_norm=True, name=''):

    # If downsample is True, apply max-pooling
    if downsample:
        x = tfkl.MaxPooling2D(name='MaxPool_' + name)(x)

    # Apply a stack of convolutional layers with optional batch normalization and specified activation
    for s in range(stack):
        x = tfkl.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, name='Conv_' + name + str(s+1))(x)
        if batch_norm:
            x = tfkl.BatchNormalization(name='BatchNorm_' + name + str(s+1))(x)
        x = tfkl.Activation(activation, name='Activation_' + name + str(s+1))(x)

    return x

# Define a function to build a VGG18-like model with batch normalization
def build_bn_model(input_shape=input_shape, output_shape=output_shape, learning_rate=1e-4, seed=seed):

    # Input layer
    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')

    # Define a preprocessing Sequential model with random flip, zero padding, and random crop
    preprocessing = tfk.Sequential([
        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
        tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
        tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop')
    ], name='Preprocessing')

    # Apply preprocessing to the input layer
    x0 = preprocessing(input_layer)

    # Initial convolution with batch normalization and activation
    x0 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)
    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)
    x0 = tfkl.Activation('relu', name='ReLU0')(x0)

    # Create convolutional blocks with batch normalization
    x1 = conv_bn_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='1')
    x1 = conv_bn_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='2')

    x2 = conv_bn_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='3')
    x2 = conv_bn_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='4')

    x3 = conv_bn_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='5')
    x3 = conv_bn_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='6')

    x4 = conv_bn_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='7')
    x4 = conv_bn_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='8')

    # Global Average Pooling and classifier
    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)
    x = tfkl.Dense(output_shape, name='Dense')(x)
    output_activation = tfkl.Activation('softmax', name='Softmax')(x)

    # Create the model
    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_BN')

    # Define optimizer, loss, and metrics
    # AdamW is an Adam optimizer which applies weight_decay to network layers,
    # i.e it's another way to apply l2 regularization to the whole network
    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)
    loss = tfk.losses.CategoricalCrossentropy()
    metrics = ['accuracy']

    # Compile the model
    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
    return model

bn_model = build_bn_model()
bn_model.summary()
tfk.utils.plot_model(bn_model, expand_nested=True, show_shapes=True)

bn_history = bn_model.fit(
    X_train,
    y_train,
    validation_data=(X_val,y_val),
    epochs=epochs,
    batch_size=batch_size,
    verbose=1,
    callbacks=callbacks
).history

model.save('VGG_like_batch_norm')

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(bn_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(bn_history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(bn_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(bn_history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = bn_model.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## Previous arch with class balancing"""

# Define a function to build a VGG18-like model with batch normalization
def build_bn_model(input_shape=input_shape, output_shape=output_shape, learning_rate=1e-4, seed=seed):

    # Input layer
    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')

    # Define a preprocessing Sequential model with random flip, zero padding, and random crop
    preprocessing = tfk.Sequential([
        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
        tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
        tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop')
    ], name='Preprocessing')

    # Apply preprocessing to the input layer
    x0 = preprocessing(input_layer)

    # Initial convolution with batch normalization and activation
    x0 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)
    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)
    x0 = tfkl.Activation('relu', name='ReLU0')(x0)

    # Create convolutional blocks with batch normalization
    x1 = conv_bn_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='1')
    x1 = conv_bn_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='2')

    x2 = conv_bn_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='3')
    x2 = conv_bn_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='4')

    x3 = conv_bn_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='5')
    x3 = conv_bn_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='6')

    x4 = conv_bn_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='7')
    x4 = conv_bn_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='8')

    # Global Average Pooling and classifier
    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)
    x = tfkl.Dense(output_shape, name='Dense')(x)
    output_activation = tfkl.Activation('softmax', name='Softmax')(x)

    # Create the model
    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_BN')

    # Define optimizer, loss, and metrics
    # AdamW is an Adam optimizer which applies weight_decay to network layers,
    # i.e it's another way to apply l2 regularization to the whole network
    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)
    loss = tfk.losses.CategoricalCrossentropy()
    metrics = ['accuracy']

    # Compile the model
    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
    return model

balanced_bn_model = build_bn_model()
balanced_bn_model.summary()
tfk.utils.plot_model(balanced_bn_model, expand_nested=True, show_shapes=True)

callbacks=[
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True, mode='max')
]

balanced_bn_history = balanced_bn_model.fit(
    X_train,
    y_train,
    validation_data=(X_val,y_val),
    epochs=epochs,
    batch_size=batch_size,
    class_weight=class_weights,
    verbose=1,
    callbacks=callbacks
).history

balanced_bn_model.save('VGG_like_batch_norm_balanced')

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(balanced_bn_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(balanced_bn_history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(balanced_bn_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(balanced_bn_history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = balanced_bn_model.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## Class Balancing e more data augmentation

Mixuo
"""

class MixupLayer(Layer):
    def __init__(self, alpha=0.2, **kwargs):
        super(MixupLayer, self).__init__(**kwargs)
        self.alpha = alpha

    def call(self, inputs, training=None):
        # Assuming inputs is a tuple (x, y)
        x, y = inputs

        if training:
            batch_size = tf.shape(x)[0]
            weight = tf.random.uniform(shape=(batch_size, 1))
            index = tf.random.shuffle(tf.range(batch_size))
            x_mixup = weight * x + (1 - weight) * tf.gather(x, index)
            y_mixup = weight * y + (1 - weight) * tf.gather(y, index)
            return x_mixup, y_mixup
        else:
            return x, y

    def get_config(self):
        config = super(MixupLayer, self).get_config()
        config.update({'alpha': self.alpha})
        return config

# Example usage in a model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

# Assuming input_shape and num_classes are defined
input_layer = Input(shape=input_shape)
output_layer = Dense(2, activation='softmax')(input_layer)

# Use MixupLayer as the first layer in your model
mixed_input, mixed_label = MixupLayer(alpha=0.2)([input_layer, output_layer])

# Continue with the rest of your model architecture
# ...

# Compile your model
model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train your model with mixup
# model.fit(...)

# Define a function to build a VGG18-like model with batch normalization
def build_bn_model(input_shape=input_shape, output_shape=output_shape, learning_rate=1e-4, seed=seed):

    # Input layer
    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')

    # Define a preprocessing Sequential model with random flip, zero padding, and random crop
    preprocessing = tfk.Sequential([
        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
        tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
        tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
        tfkl.RandomRotation(0.2),
        tfkl.RandomZoom(0.2),
        tfkl.RandomContrast(0.75),
    ], name='Preprocessing')

    # Apply preprocessing to the input layer
    x0 = preprocessing(input_layer)

    # Initial convolution with batch normalization and activation
    x0 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)
    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)
    x0 = tfkl.Activation('relu', name='ReLU0')(x0)

    # Create convolutional blocks with batch normalization
    x1 = conv_bn_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='1')
    x1 = conv_bn_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='2')

    x2 = conv_bn_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='3')
    x2 = conv_bn_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='4')

    x3 = conv_bn_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='5')
    x3 = conv_bn_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='6')

    x4 = conv_bn_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='7')
    x4 = conv_bn_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='8')

    # Global Average Pooling and classifier
    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)
    x = tfkl.Dense(output_shape, name='Dense')(x)
    output_activation = tfkl.Activation('softmax', name='Softmax')(x)

    # Create the model
    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_BN')

    # Define optimizer, loss, and metrics
    # AdamW is an Adam optimizer which applies weight_decay to network layers,
    # i.e it's another way to apply l2 regularization to the whole network
    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)
    loss = tfk.losses.CategoricalCrossentropy()
    metrics = ['accuracy']

    # Compile the model
    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
    return model

balanced_bn_model = build_bn_model()
balanced_bn_model.summary()
tfk.utils.plot_model(balanced_bn_model, expand_nested=True, show_shapes=True)

callbacks=[
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True, mode='max')
]

balanced_bn_history = balanced_bn_model.fit(
    X_train,
    y_train,
    validation_data=(X_val,y_val),
    epochs=epochs,
    batch_size=batch_size,
    class_weight=class_weights,
    verbose=1,
    callbacks=callbacks
).history

balanced_bn_model.save('VGG_like_batch_norm_balanced_and_more_augmentation')

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(balanced_bn_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(balanced_bn_history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(balanced_bn_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(balanced_bn_history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = balanced_bn_model.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

optimal_epochs = 353 - 100 # sosituire
balanced_bn_history = balanced_bn_model.fit(
    X,
    y,
    epochs=optimal_epochs,
    batch_size=batch_size,
    class_weight=class_weights,
    verbose=1,
).history

balanced_bn_model.save("Balanced_BN_Model_more_aug_final")

del balanced_bn_model
balanced_bn_model = tf.keras.models.load_model("Balanced_BN_Model_more_aug_final")

out = balanced_bn_model.predict(X_test)
out = tf.argmax(out, axis=-1)

"""## Previus arch with tweaked hyperparameters

let's try to tweak the hyperparameters.
"""

# Define batch size, number of epochs, learning rate, input shape, and output shape
batch_size = 256
epochs = 1000
learning_rate = 1e-2
input_shape = X_train.shape[1:]
output_shape = y_train.shape[-1]

# Print batch size, epochs, learning rate, input shape, and output shape
print(f"Batch Size: {batch_size}, Epochs: {epochs}, Learning Rate: {learning_rate}")
print(f"Input Shape: {input_shape}, Output Shape: {output_shape}")

callbacks=[
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True, mode='max'),
    tfk.callbacks.ReduceLROnPlateau(monitor="val_accuracy", factor=0.1, patience=20, min_lr=1e-5, mode='max')
]

# Define a function to build a VGG18-like model with batch normalization
def build_bn_model(input_shape=input_shape, output_shape=output_shape, learning_rate=learning_rate, seed=seed):

    # Input layer
    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')

    # Define a preprocessing Sequential model with random flip, zero padding, and random crop
    preprocessing = tfk.Sequential([
        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
        tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
        tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop')
    ], name='Preprocessing')

    # Apply preprocessing to the input layer
    x0 = preprocessing(input_layer)

    # Initial convolution with batch normalization and activation
    x0 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)
    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)
    x0 = tfkl.Activation('relu', name='ReLU0')(x0)

    # Create convolutional blocks with batch normalization
    x1 = conv_bn_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='1')
    x1 = conv_bn_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='2')

    x2 = conv_bn_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='3')
    x2 = conv_bn_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='4')

    x3 = conv_bn_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='5')
    x3 = conv_bn_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='6')

    x4 = conv_bn_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, batch_norm=True, name='7')
    x4 = conv_bn_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, batch_norm=True, name='8')

    # Global Average Pooling and classifier
    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)
    x = tfkl.Dense(output_shape, name='Dense')(x)
    output_activation = tfkl.Activation('softmax', name='Softmax')(x)

    # Create the model
    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_BN')

    # Define optimizer, loss, and metrics
    # AdamW is an Adam optimizer which applies weight_decay to network layers,
    # i.e it's another way to apply l2 regularization to the whole network
    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)
    loss = tfk.losses.CategoricalCrossentropy()
    metrics = ['accuracy']

    # Compile the model
    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
    return model

bn_model2 = build_bn_model()
bn_model2.summary()
tfk.utils.plot_model(bn_model2, expand_nested=True, show_shapes=True)

bn_history2 = bn_model2.fit(
    X_train,
    y_train,
    validation_data=(X_val,y_val),
    epochs=epochs,
    batch_size=batch_size,
    verbose=1,
    callbacks=callbacks
).history

model.save('VGG_like_batch_norm_2')

# Plot the training
plt.figure(figsize=(15,5))
plt.plot(bn_history2['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(bn_history2['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(bn_history2['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(bn_history2['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = bn_model2.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## Skip Connections"""

# Define a residual convolutional block with optional batch normalization
def conv_residual_block(x, filters, kernel_size, padding='same', downsample=True, activation='relu', stack=2, batch_norm=True, name=''):

    # If downsample is True, apply max-pooling
    if downsample:
        x = tfkl.MaxPooling2D(name='MaxPool_' + name)(x)

    # Create a copy of the input for the residual connection
    x_ = x

    # Apply a stack of convolutional layers to the copy
    for s in range(stack):
        x_ = tfkl.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, name='Conv_' + name + str(s+1))(x_)
        if batch_norm:
            x_ = tfkl.BatchNormalization(name='BatchNorm_' + name + str(s+1))(x_)
        x_ = tfkl.Activation(activation, name='Activation_' + name + str(s+1))(x_)

    # If downsample is True, apply a 1x1 convolution to match the number of filters
    if downsample:
        x = tfkl.Conv2D(filters=filters, kernel_size=1, padding=padding, name='Conv_' + name + 'skip')(x)

    # Add the original and the processed copy to create the residual connection
    x = tfkl.Add(name='Add_' + name)([x_, x])

    return x

# Define a function to build a VGG18-like model with residual blocks
def build_model_residual(input_shape=input_shape, output_shape=output_shape, learning_rate=1e-4, seed=seed):

    # Input layer
    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')

    # Initial convolution with batch normalization and activation
    x0 = tfkl.Conv1D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)
    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)
    x0 = tfkl.Activation('relu', name='ReLU0')(x0)

    # Create residual blocks
    x1 = conv_residual_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, name='1')
    x1 = conv_residual_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, name='2')

    x2 = conv_residual_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, name='3')
    x2 = conv_residual_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, name='4')

    x3 = conv_residual_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, name='5')
    x3 = conv_residual_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, name='6')

    x4 = conv_residual_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, name='7')
    x4 = conv_residual_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, name='8')

    # Global Average Pooling and classifier
    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)
    x = tfkl.Dense(output_shape, name='Dense')(x)
    output_activation = tfkl.Activation('softmax', name='Softmax')(x)

    # Create the model
    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_Residual')

    # Define optimizer, loss, and metrics
    # AdamW is an Adam optimizer which applies weight_decay to network layers,
    # i.e it's another way to apply l2 regularization to the whole network
    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)
    loss = tfk.losses.CategoricalCrossentropy()
    metrics = ['accuracy']

    # Compile the model
    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
    return model

# Define a function to build a VGG18-like model with residual blocks
def build_model_residual(input_shape=input_shape, output_shape=output_shape, learning_rate=1e-4, seed=seed):

    # Input layer
    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')

    # Define a preprocessing Sequential model with random flip, zero padding, and random crop
    preprocessing = tfk.Sequential([
        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
        tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
        tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
        tfkl.RandomRotation(0.2),
        tfkl.RandomZoom(0.2),
        tfkl.RandomContrast(0.75),
    ], name='Preprocessing')

    # Apply preprocessing to the input layer
    x0 = preprocessing(input_layer)

    # Initial convolution with batch normalization and activation
    x0 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)
    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)
    x0 = tfkl.Activation('relu', name='ReLU0')(x0)

    # Create residual blocks
    x1 = conv_residual_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, name='1')
    x1 = conv_residual_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, name='2')

    x2 = conv_residual_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, name='3')
    x2 = conv_residual_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, name='4')

    x3 = conv_residual_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, name='5')
    x3 = conv_residual_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, name='6')

    x4 = conv_residual_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, name='7')
    x4 = conv_residual_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, name='8')

    # Global Average Pooling and classifier
    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)
    x = tfkl.Dense(output_shape, name='Dense')(x)
    output_activation = tfkl.Activation('softmax', name='Softmax')(x)

    # Create the model
    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_Residual')

    # Define optimizer, loss, and metrics
    # AdamW is an Adam optimizer which applies weight_decay to network layers,
    # i.e it's another way to apply l2 regularization to the whole network
    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)
    loss = tfk.losses.CategoricalCrossentropy()
    metrics = ['accuracy']

    # Compile the model
    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
    return model

residual_model = build_model_residual()
residual_model.summary()
tfk.utils.plot_model(residual_model, expand_nested=True, show_shapes=True)

callbacks=[
    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True, mode='max'),
    tfk.callbacks.ReduceLROnPlateau(monitor="val_accuracy", factor=0.1, patience=20, min_lr=1e-5, mode='max')
]

residual_history = residual_model.fit(
    X_train,
    y_train,
    validation_data=(X_val,y_val),
    epochs=epochs,
    batch_size=batch_size,
    class_weight=class_weights,
    verbose=1,
    callbacks=callbacks
).history

# Plot validation loss for different models
plt.figure(figsize=(15, 3))
plt.plot(residual_history['loss'], label='VGG18+BN+skip', alpha=0.8, color='#ff7f0e')
plt.title('Loss')
plt.plot(residual_history['val_loss'], label='VGG18+BN+skip', alpha=0.8, color='#2ca02c')
plt.title('Validation Loss')
plt.legend()
plt.grid(alpha=0.3)

# Plot validation accuracy for different models
plt.figure(figsize=(15, 3))
plt.plot(residual_history['accuracy'], label='VGG18+BN+skip', alpha=0.8, color='#ff7f0e')
plt.title('Accuracy')
plt.plot(residual_history['val_accuracy'], label='VGG18+BN+skip', alpha=0.8, color='#2ca02c')
plt.title('Validation Accuracy')
plt.legend()
plt.grid(alpha=0.3)

# Show the plots
plt.show()

residual_model.save('residual_model')

# Predict labels for the entire test set
predictions = residual_model.predict(X_test, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, cmap='Blues')
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

callbacks=[
    tfk.callbacks.ReduceLROnPlateau(monitor="val_accuracy", factor=0.1, patience=20, min_lr=1e-5, mode='max')
]

optimal_epochs = 157-50

final_res_model = build_model_residual()

residual_history = final_res_model.fit(
    X,
    y,
    epochs=optimal_epochs,
    batch_size=batch_size,
    class_weight=class_weights,
    verbose=1,
    callbacks=callbacks
).history

final_res_model.save('final_residual_model')

# Plot validation loss for different models
plt.figure(figsize=(15, 3))
plt.plot(residual_history['loss'], label='VGG18+BN+skip', alpha=0.8, color='#ff7f0e')
plt.title('Loss')
plt.legend()
plt.grid(alpha=0.3)

# Plot validation accuracy for different models
plt.figure(figsize=(15, 3))
plt.plot(residual_history['accuracy'], label='VGG18+BN+skip', alpha=0.8, color='#ff7f0e')
plt.title('Accuracy')
plt.legend()
plt.grid(alpha=0.3)

# Show the plots
plt.show()

"""## Stochasting Pooling [TRY]"""

import tensorflow as tf
import numpy as np

class StochasticPooling2D(tf.keras.layers.Layer):
    def __init__(self, pool_size=(2, 2), **kwargs):
        super(StochasticPooling2D, self).__init__(**kwargs)
        self.pool_size = pool_size

    def call(self, inputs, training=None):
        if training is None:
            training = tf.keras.backend.learning_phase()

        if training:
            batch_size, rows, cols, channels = inputs.shape
            pool_height, pool_width = self.pool_size

            # Reshape the input tensor to perform pooling
            inputs_reshaped = tf.reshape(inputs, (batch_size, rows, cols, pool_height, pool_width, channels))

            # Calculate probabilities for selecting each element
            probabilities = tf.math.softmax(inputs_reshaped, axis=4)

            # Generate random indices based on probabilities
            random_indices = tf.random.categorical(tf.math.log(probabilities), num_samples=1)

            # Flatten the selected indices
            random_indices = tf.squeeze(random_indices, axis=4)

            # Gather the values based on the random indices
            pooled_outputs = tf.gather(inputs_reshaped, random_indices, axis=4)

            # Reshape back to the original shape
            pooled_outputs = tf.reshape(pooled_outputs, (batch_size, rows // pool_height, cols // pool_width, channels))
        else:
            # Inference mode: Use standard max pooling
            pooled_outputs = tf.keras.layers.MaxPooling2D(pool_size=self.pool_size)(inputs)

        return pooled_outputs

    def compute_output_shape(self, input_shape):
        batch_size, rows, cols, channels = input_shape
        pool_height, pool_width = self.pool_size
        return (batch_size, rows // pool_height, cols // pool_width, channels)

# Example usage in a model
model = tf.keras.Sequential()
model.add(StochasticPooling2D(pool_size=(2, 2), input_shape=(32, 32, 3))
# Add other layers...

"""# Transfer Learning

## Data Import and Preprocessing

### importing
"""

# Fix randomness and hide warnings
seed = 42

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=Warning)

import numpy as np
np.random.seed(seed)

import logging

import random
random.seed(seed)

# Import tensorflow
import tensorflow as tf
from tensorflow import keras as tfk
from tensorflow.keras import layers as tfkl
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel(logging.ERROR)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)
print(tf.__version__)

# Import other libraries
import cv2
from tensorflow.keras.applications.mobilenet import preprocess_input
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
import seaborn as sns

import pandas as pd

"""### load data"""

public_data = np.load('public_data.npz', allow_pickle=True)

data = public_data['data']/255
labels = public_data['labels']

print(data.shape)
print(labels.shape)

"""We have 5200 samples of 96x96 images encoded in RGB."""

categories = np.unique(labels)
print(categories)

c_h = 0
c_u = 0
for label in labels:
  if label=='healthy':
    c_h += 1
  elif label=='unhealthy':
    c_u += 1

print(c_h)
print(c_u)

class_weights = [c_h/data.shape[0], c_u/data.shape[0]]
print(class_weights)

class_weights = {0: class_weights[0], 1: class_weights[1]}
print(class_weights)

"""### visualize data and data cleaning"""

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(data[i])
  titolo = labels[i]
  ax.set_title(titolo)

"""Finding Shreks"""

first_shrek = data[95]
shreks = list()
shreks_labels = list()
shreks_indices = list()

for i in range(0,data.shape[0]):
  if (data[i] == first_shrek).all():
    shreks.append(data[i])
    shreks_labels.append(labels[i])
    shreks_indices.append(i)

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(shreks[i])
  ax.set_title(shreks_labels[i])

"""Trollolos"""

first_tro = data[338]
tro = list()
tro_labels = list()
tro_indices = list()

for i in range(0,data.shape[0]):
  if (data[i] == first_tro).all():
    tro.append(data[i])
    tro_labels.append(labels[i])
    tro_indices.append(i)

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(tro[i])
  ax.set_title(tro_labels[i])

"""drop shreks and trollolos from the dataset"""

bad_data_indices = shreks_indices + tro_indices

data = np.delete(data, bad_data_indices, axis = 0)
labels = np.delete(labels, bad_data_indices)
data.shape

healthy = data[labels=='healthy']
unhealthy = data[labels=='unhealthy']

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(healthy[i])
  ax.set_title('healthy')

fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(unhealthy[i])
  ax.set_title('unhealthy')

"""batch_size = 100  # Adjust as needed
for i in range(0, len(data), batch_size):
    batch = data[i:i + batch_size]

    # Display images in the current batch
    for j, image in enumerate(batch):
        plt.subplot(10, 10, j + 1)  # Adjust subplot parameters as needed
        plt.imshow(image)
        #titolo = labels[i]
        plt.axis('off')
        plt.title(str(i + j))

    plt.show()

save the cleaned dataset in npz format
"""

np.savez('cleaned_public_data.npz', array1=data, array2=labels)

c_h = 0
c_u = 0
for label in labels:
  if label=='healthy':
    c_h += 1
  elif label=='unhealthy':
    c_u += 1

print(c_h)
print(c_u)

class_weights = [c_h/data.shape[0], c_u/data.shape[0]]
print(class_weights)

class_weights = {0: class_weights[0], 1: class_weights[1]}
print(class_weights)

"""### split

Now we use one-hot encoding to our categorical labels to be able to feed them to the network. Maybe we could avoid one-hot since we onlky have 2 classes and turn this into a regression problem.
"""

# Convert to a pandas Series
labels_series = pd.Series(labels)

# Convert to categorical variable
categorical_labels = labels_series.astype('category')

# Convert to category codes
category_codes = categorical_labels.cat.codes

# Use tfk.utils.to_categorical to one-hot encode the category codes
y = tfk.utils.to_categorical(category_codes, num_classes=len(categorical_labels.cat.categories))

# The result is a one-hot encoded representation of your labels
print("One-Hot Encoded Labels:")
print(y)

"""And we split our dataset into training, validation and test set."""

X = data

# Split data into train_val and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=.25, stratify=np.argmax(y,axis=1))

# Further split train_val into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=np.argmax(y_train_val,axis=1))

# Print shapes of the datasets
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

"""Ok, now we have a training set, a validation set and a test set.

### build model

NB. for the function build_model we have to use the output_shape without expanding dimensions
"""

# Define input shape, output shape, batch size, and number of epochs
input_shape = X_train.shape[1:]
output_shape_not_expanded = y_train.shape[1:]
output_shape = np.expand_dims(output_shape_not_expanded, axis=-1)
batch_size = 32
epochs = 1000

# Print input shape, batch size, and number of epochs
print(f"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}")

learning_rate = 1e-2

"""First we want to build our model and see if it's able to overfit the data, since we want to see if it can effectively learn the problem.

## EfficientNet B0 v2
"""

# Create MobileNetV2 model with specified settings
model = tfk.applications.EfficientNetV2B0(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
    include_preprocessing=True
)

# Display the model architecture with input shapes
tfk.utils.plot_model(model, show_shapes=True)

len(model.layers)

# Create an input layer
inputs = tfk.Input(shape=input_shape)

# Define a preprocessing Sequential model with random flip, zero padding, and random crop
"""
preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     tfkl.RandomZoom(0.2),
     tfkl.RandomContrast(0.75),
], name='Preprocessing')
"""

# x = preprocessing(inputs)


# Connect the net to the input
x = model(inputs)
# Add a Dense layer with 2 units and softmax activation as the classifier
x = tfkl.Dense(64, activation='relu')(x)
x = tfkl.Dense(64, activation='relu')(x)
outputs = tfkl.Dense(2, activation='softmax')(x)

# Create a Model connecting input and output
model = tfk.Model(inputs=inputs, outputs=outputs, name='model')


# =============================
# TRAINABLE WEIGHTS

# Set all layers as trainable
model.get_layer('efficientnetv2-b0').trainable = True
for i, layer in enumerate(model.get_layer('efficientnetv2-b0').layers):
   print(i, layer.name, layer.trainable)

# Freeze first N layers, e.g., until the 133rd one
N = 270
for i, layer in enumerate(model.get_layer('efficientnetv2-b0').layers[:N]):
  layer.trainable=False
for i, layer in enumerate(model.get_layer('efficientnetv2-b0').layers):
   print(i, layer.name, layer.trainable)

# =============================


# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
history = model.fit(
    x = X_train*255,
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val),
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]
).history

# Plot the re-trained and the transfer learning MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Save the best model
model.save('EfficientNetv0b2_model')
del model

"""## ResNet50"""

# Import other libraries
from tensorflow.keras.applications.resnet import preprocess_input

# Create MobileNetV2 model with specified settings
model = tfk.applications.ResNet50(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
)

# Display the model architecture with input shapes
tfk.utils.plot_model(model, show_shapes=True)

len(model.layers)

# Create an input layer
inputs = tfk.Input(shape=input_shape)

# Define a preprocessing Sequential model with random flip, zero padding, and random crop
preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     tfkl.RandomZoom(0.2),
     tfkl.RandomContrast(0.75),
], name='Preprocessing')

x = preprocessing(inputs)


# Connect the net to the input
x = model(x)
# Add a Dense layer with 2 units and softmax activation as the classifier
x = tfkl.Dense(512, activation='relu')(x)
x = tfkl.Dense(128, activation='relu')(x)
outputs = tfkl.Dense(2, activation='softmax')(x)

# Create a Model connecting input and output
model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

print(model.layers)

# =============================
# TRAINABLE WEIGHTS
name = 'resnet50'
# Set all layers as trainable
model.get_layer(name).trainable = True
for i, layer in enumerate(model.get_layer(name).layers):
   print(i, layer.name, layer.trainable)

# Freeze first N layers, e.g., until the 133rd one
N = 100
for i, layer in enumerate(model.get_layer(name).layers[:N]):
  layer.trainable=False
for i, layer in enumerate(model.get_layer(name).layers):
   print(i, layer.name, layer.trainable)
# =============================


# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
history = model.fit(
    x = preprocess_input(X_train), # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 32,
    epochs = 200,
    validation_data = (preprocess_input(X_val), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]
).history

# Plot the re-trained and the transfer learning MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Save the best model
model.save('ResNet50_model')
del model

"""## MobileNet"""

# Import other libraries
from tensorflow.keras.applications.mobilenet import preprocess_input

# Create MobileNetV2 model with specified settings
model = tfk.applications.MobileNetV2(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
)

# Display the model architecture with input shapes
tfk.utils.plot_model(model, show_shapes=True)

len(model.layers)

# Create an input layer
inputs = tfk.Input(shape=input_shape)

"""
# Define a preprocessing Sequential model with random flip, zero padding, and random crop
preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     tfkl.RandomZoom(0.2),
     tfkl.RandomContrast(0.75),
], name='Preprocessing')


x = preprocessing(inputs)
"""


# Connect the net to the input
x = model(inputs)
# Add a Dense layer with 2 units and softmax activation as the classifier
x = tfkl.Dense(512, activation='relu')(x)
x = tfkl.Dense(128, activation='relu')(x)
outputs = tfkl.Dense(2, activation='softmax')(x)

# Create a Model connecting input and output
model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

print(model.layers)

# =============================
# TRAINABLE WEIGHTS
name = 'mobilenetv2_1.00_96'
# Set all layers as trainable
model.get_layer(name).trainable = True
for i, layer in enumerate(model.get_layer(name).layers):
   print(i, layer.name, layer.trainable)

# Freeze first N layers, e.g., until the 133rd one
N = 0
for i, layer in enumerate(model.get_layer(name).layers[:N]):
  layer.trainable=False
for i, layer in enumerate(model.get_layer(name).layers):
   print(i, layer.name, layer.trainable)
# =============================


# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
history = model.fit(
    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 32,
    epochs = 200,
    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]
).history

# Plot the re-trained and the transfer learning MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Save the best model
model.save('MobileNet_model')
del model

"""## EfficientNet"""

mobile = tfk.applications.EfficientNetV2S(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
    include_preprocessing=True,
)
#tfk.utils.plot_model(mobile, show_shapes=True)

"""# Use the supernet as feature extractor, i.e. freeze all its weigths
mobile.trainable = False

# Create an input layer with shape (224, 224, 3)
inputs = tfk.Input(shape=input_shape)
# Connect MobileNetV2 to the input
x = mobile(inputs)
# Add a Dense layer with 2 units and softmax activation as the classifier
outputs = tfkl.Dense(, activation='softmax')(x)

# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])

# Display model summary
tl_model.summary()"""

# Use the supernet as feature extractor, i.e. freeze all its weigths
mobile.trainable = False

# Create an input layer with shape (224, 224, 3)
inputs = tfk.Input(shape=input_shape)

preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     #tfkl.RandomZoom(0.2),
     #tfkl.RandomContrast(0.75),
], name='Preprocessing')

x = preprocessing(inputs)

# Connect MobileNetV2 to the input
x = mobile(x)

# Classifier Dense Network
x = tfkl.Dense(512, activation='relu')(x)
x = tfkl.Dropout(0.5)(x)
outputs = tfkl.Dense(2, activation='softmax')(x)

# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])

# Display model summary
tl_model.summary()

# Train the model
tl_history = tl_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Save the best model
tl_model.save('efficientnetv2_with_dropout_lr_scheduling')
#del tl_model

# Plot the re-trained and the transfer learning MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = tl_model.predict(X_test*255, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""retrain with fixed hyperparameters on whole dataset"""

mobile = tfk.applications.EfficientNetV2S(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
    include_preprocessing=True,
)
#tfk.utils.plot_model(mobile, show_shapes=True)

# Use the supernet as feature extractor, i.e. freeze all its weigths
mobile.trainable = False

# Create an input layer with shape (224, 224, 3)
inputs = tfk.Input(shape=input_shape)

preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     #tfkl.RandomZoom(0.2),
     #tfkl.RandomContrast(0.75),
], name='Preprocessing')

x = preprocessing(inputs)

# Connect MobileNetV2 to the input
x = mobile(x)

# Classifier Dense Network
x = tfkl.Dense(512, activation='relu')(x)
x = tfkl.Dropout(0.5)(x)
outputs = tfkl.Dense(2, activation='softmax')(x)

# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])

# Display model summary
tl_model.summary()

# Train the model
tl_history = tl_model.fit(
    x = X_train_val*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train_val,
    batch_size = 16,
    epochs = 78,
    validation_data = (X_test*255, y_test), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Plot the re-trained and the transfer learning MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Save the best model
tl_model.save('efficientnetv2_with_dropout_lr_scheduling_final')
#del tl_model

"""## K-Fold cross Validation"""

def build_model(input_shape=input_shape):
  mobile = tfk.applications.EfficientNetV2S(
      input_shape=input_shape,
      include_top=False,
      weights="imagenet",
      pooling='avg',
      include_preprocessing=True,
  )
  #tfk.utils.plot_model(mobile, show_shapes=True)

  # Use the supernet as feature extractor, i.e. freeze all its weigths
  mobile.trainable = False

  # Create an input layer with shape (224, 224, 3)
  inputs = tfk.Input(shape=input_shape)

  preprocessing = tfk.Sequential([
      tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
      #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
      #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
      tfkl.RandomRotation(0.2),
      #tfkl.RandomZoom(0.2),
      #tfkl.RandomContrast(0.75),
  ], name='Preprocessing')

  x = preprocessing(inputs)

  # Connect MobileNetV2 to the input
  x = mobile(x)

  # Classifier Dense Network
  x = tfkl.Dense(512, activation='relu')(x)
  x = tfkl.Dropout(0.5)(x)
  outputs = tfkl.Dense(2, activation='softmax')(x)

  # Create a Model connecting input and output
  tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

  # Compile the model with Categorical Cross-Entropy loss and Adam optimizer
  tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate=initial_learning_rate), metrics=['accuracy','mse'])

  # Display model summary
  tl_model.summary()

  return tl_model

# Import other libraries
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
plt.rc('font', size=16)
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error

# Define the number of folds for cross-validation
num_folds = 10

# Initialize lists to store training histories, scores, and best epochs
histories = []
scores = []
best_epochs = []

# Create a KFold cross-validation object
kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)

# Loop through each fold
for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(X_train_val, y_train_val)):

  print("Starting training on fold num: {}".format(fold_idx+1))

  # Build a new dropout model for each fold
  k_model = build_model(input_shape)

  # Train the model on the training data for this fold
  history = k_model.fit(
    x = X_train_val[train_idx],
    y = y_train_val[train_idx],
    validation_data=(X_train_val[valid_idx], y_train_val[valid_idx]),
    batch_size = batch_size,
    epochs = epochs,
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler],
    verbose = 0
  ).history

  # Evaluate the model on the validation data for this fold
  score = k_model.evaluate(X_train_val[valid_idx], y_train_val[valid_idx], verbose=0)
  scores.append(score[1])

  # Calculate the best epoch for early stopping
  best_epoch = len(history['loss']) - patience
  best_epochs.append(best_epoch)

  # Store the training history for this fold
  histories.append(history)

# Define a list of colors for plotting
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']

# Print mean and standard deviation of MSE scores
print("MSE")
print(f"Mean: {np.mean(scores).round(4)}\nStd:  {np.std(scores).round(4)}")

# Create a figure for MSE visualization
plt.figure(figsize=(15,6))

# Plot MSE for each fold
for fold_idx in range(num_folds):
  plt.plot(histories[fold_idx]['val_mse'][:-patience], color=colors[fold_idx], label=f'Fold N¬∞{fold_idx+1}')
  plt.ylim(0.009, 0.02)
  plt.title('Mean Squared Error')
  plt.legend(loc='upper right')
  plt.grid(alpha=.3)

# Show the plot
plt.show()

# Calculate the average best epoch
avg_epochs = int(np.mean(best_epochs))
print(f"Best average epoch: {avg_epochs}")

# Build the final model using the calculated average best epoch
final_model = build_model(input_shape)

# Train the final model on the combined training and validation data
final_history = final_model.fit(
    x = X_train_val,
    y = y_train_val,
    batch_size = batch_size,
    epochs = avg_epochs
).history

# Evaluate and plot the performance of the final model on the test data
print('Final Model Test Performance')
plot_residuals(final_model, X_test.copy(), y_test.copy())

"""FINE TUNING"""

# Re-load the model after transfer learning
ft_model = tfk.models.load_model('EfficientNetV2S')
ft_model.summary()

len(mobile.layers)
mobile.summary()

# Set all EfficientNetV2S layers as trainable
ft_model.get_layer('efficientnetv2-s').trainable = True
for i, layer in enumerate(ft_model.get_layer('efficientnetv2-s').layers):
   print(i, layer.name, layer.trainable)

# Freeze first N layers, e.g., until the 133rd one
N = 400
for i, layer in enumerate(ft_model.get_layer('efficientnetv2-s').layers[:N]):
  layer.trainable=False
for i, layer in enumerate(ft_model.get_layer('efficientnetv2-s').layers):
   print(i, layer.name, layer.trainable)
ft_model.summary()

# Compile the model
ft_model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')

# Fine-tune the model
ft_history = ft_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]
).history

# Plot the re-trained, the transfer learning and the fine-tuned MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.plot(ft_history['loss'], alpha=.3, color='#408537', linestyle='--')
plt.plot(ft_history['val_loss'], label='Fine Tuning', alpha=.8, color='#408537')
plt.legend(loc='upper left')
plt.title('Binary Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')
plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.plot(ft_history['accuracy'], alpha=.3, color='#408537', linestyle='--')
plt.plot(ft_history['val_accuracy'], label='Fine Tuning', alpha=.8, color='#408537')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Evaluate the model on the test set
test_accuracy = ft_model.evaluate(preprocess_input(X_test*255),y_test,verbose=0)[-1]
print('Test set accuracy %.4f' % test_accuracy)

# Save the best model
ft_model.save('efficientnetv2-s')
del ft_model

"""## ResNet"""

# Import other libraries
from tensorflow.keras.applications.resnet import preprocess_input

mobile = tfk.applications.ResNet50(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
    classes=2
)
tfk.utils.plot_model(mobile, show_shapes=True)

"""# Use the supernet as feature extractor, i.e. freeze all its weigths
mobile.trainable = False

# Create an input layer with shape (224, 224, 3)
inputs = tfk.Input(shape=input_shape)
# Connect MobileNetV2 to the input
x = mobile(inputs)
# Add a Dense layer with 2 units and softmax activation as the classifier
outputs = tfkl.Dense(, activation='softmax')(x)

# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])

# Display model summary
tl_model.summary()"""

# Use the supernet as feature extractor, i.e. freeze all its weigths
mobile.trainable = False

# Create an input layer with shape (224, 224, 3)
inputs = tfk.Input(shape=input_shape)

preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     #tfkl.RandomZoom(0.2),
     #tfkl.RandomContrast(0.75),
], name='Preprocessing')

x = preprocessing(inputs)

# Connect MobileNetV2 to the input
x = mobile(x)

# Classifier Dense Network
x = tfkl.Dense(512, activation='relu')(x)
x = tfkl.Dropout(0.5)(x)
outputs = tfkl.Dense(2, activation='softmax')(x)

# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])

# Display model summary
tl_model.summary()

# Train the model
tl_history = tl_model.fit(
    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Save the best model
tl_model.save('resnet50_with_dropout_lr_scheduling')
#del tl_model

# Plot the re-trained and the transfer learning MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = tl_model.predict(X_test*255, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""retrain with fixed hyperparameters on whole dataset"""

mobile = tfk.applications.EfficientNetV2S(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
    include_preprocessing=True,
)
#tfk.utils.plot_model(mobile, show_shapes=True)

# Use the supernet as feature extractor, i.e. freeze all its weigths
mobile.trainable = False

# Create an input layer with shape (224, 224, 3)
inputs = tfk.Input(shape=input_shape)

preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     #tfkl.RandomZoom(0.2),
     #tfkl.RandomContrast(0.75),
], name='Preprocessing')

x = preprocessing(inputs)

# Connect MobileNetV2 to the input
x = mobile(x)

# Classifier Dense Network
x = tfkl.Dense(512, activation='relu')(x)
x = tfkl.Dropout(0.5)(x)
outputs = tfkl.Dense(2, activation='softmax')(x)

# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])

# Display model summary
tl_model.summary()

# Train the model
tl_history = tl_model.fit(
    x = X_train_val*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train_val,
    batch_size = 16,
    epochs = 78,
    validation_data = (X_test*255, y_test), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Plot the re-trained and the transfer learning MobileNetV2 training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Save the best model
tl_model.save('resnet_with_dropout_lr_scheduling_final')
#del tl_model

"""FINE TUNING"""

# Re-load the model after transfer learning
ft_model = tfk.models.load_model('resnet50_with_dropout_lr_scheduling')
ft_model.summary()

mobile.summary()
print(len(mobile.layers))

# Set all layers as trainable
ft_model.get_layer('resnet50').trainable = True
for i, layer in enumerate(ft_model.get_layer('resnet50').layers):
   print(i, layer.name, layer.trainable)

# Freeze first N layers, e.g., until the 133rd one
N = 100
for i, layer in enumerate(ft_model.get_layer('resnet50').layers[:N]):
  layer.trainable=False
for i, layer in enumerate(ft_model.get_layer('resnet50').layers):
   print(i, layer.name, layer.trainable)
ft_model.summary()

# Compile the model
ft_model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')

# Fine-tune the model
ft_history = ft_model.fit(
    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]
).history

# Plot the re-trained, the transfer learning and the fine-tuned training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.plot(ft_history['loss'], alpha=.3, color='#408537', linestyle='--')
plt.plot(ft_history['val_loss'], label='Fine Tuning', alpha=.8, color='#408537')
plt.legend(loc='upper left')
plt.title('Binary Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.plot(ft_history['accuracy'], alpha=.3, color='#408537', linestyle='--')
plt.plot(ft_history['val_accuracy'], label='Fine Tuning', alpha=.8, color='#408537')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Evaluate the model on the test set
test_accuracy = ft_model.evaluate(preprocess_input(X_test*255),y_test,verbose=0)[-1]
print('Test set accuracy %.4f' % test_accuracy)

# Save the best model
ft_model.save('efficientnetv2-s')
del ft_model

"""## ConvNeXt tiny"""

mobile = tf.keras.applications.ConvNeXtTiny(
    model_name="convnext_tiny",
    include_top=False,
    include_preprocessing=True,
    weights="imagenet",
    input_tensor=None,
    input_shape=input_shape,
    pooling=None,
    classes=2,
    classifier_activation="softmax",
)

tfk.utils.plot_model(mobile, show_shapes=True)

def build_model(input_shape=input_shape, mobile=mobile):

  # Use the supernet as feature extractor, i.e. freeze all its weigths
  mobile.trainable = False

  # Create an input layer with shape (224, 224, 3)
  inputs = tfk.Input(shape=input_shape)

  preprocessing = tfk.Sequential([
      tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
      #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
      #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
      tfkl.RandomRotation(0.2),
      #tfkl.RandomZoom(0.2),
      #tfkl.RandomContrast(0.75),
  ], name='Preprocessing')

  x = preprocessing(inputs)

  # Connect MobileNetV2 to the input
  x = mobile(x)

  x = tf.keras.layers.GlobalAveragePooling2D()(x)

  # Classifier Dense Network
  x = tfkl.Dense(512, activation='relu')(x)
  x = tfkl.Dropout(0.5)(x)
  outputs = tfkl.Dense(2, activation='softmax')(x)

  # Create a Model connecting input and output
  tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

  # Compile the model with Categorical Cross-Entropy loss and Adam optimizer
  tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=initial_learning_rate, weight_decay=5e-4), metrics=['accuracy','mse'])

  # Display model summary
  tl_model.summary()

  return tl_model

tl_model = build_model(input_shape, mobile)

# Train the model
tl_history = tl_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Save the best model
tl_model.save('convnet_tiny')
#del tl_model

# Plot the re-trained and the transfer learning training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['mse'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_mse'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('MSE')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = tl_model.predict(X_test*255, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## ConvNeXt small"""

mobile = tf.keras.applications.ConvNeXtSmall(
    model_name="convnext_small",
    include_top=False,
    include_preprocessing=True,
    weights="imagenet",
    input_tensor=None,
    input_shape=input_shape,
    pooling=None,
    classes=2,
    classifier_activation="softmax",
)

tfk.utils.plot_model(mobile, show_shapes=True)

def build_model(input_shape=input_shape, mobile=mobile):

  # Use the supernet as feature extractor, i.e. freeze all its weigths
  mobile.trainable = False

  # Create an input layer with shape (224, 224, 3)
  inputs = tfk.Input(shape=input_shape)

  preprocessing = tfk.Sequential([
      tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
      #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
      #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
      tfkl.RandomRotation(0.2),
      #tfkl.RandomZoom(0.2),
      #tfkl.RandomContrast(0.75),
  ], name='Preprocessing')

  x = preprocessing(inputs)

  # Connect MobileNetV2 to the input
  x = mobile(x)

  x = tf.keras.layers.GlobalAveragePooling2D()(x)

  # Classifier Dense Network
  x = tfkl.Dense(512, activation='relu')(x)
  x = tfkl.Dropout(0.5)(x)
  outputs = tfkl.Dense(2, activation='softmax')(x)

  # Create a Model connecting input and output
  tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

  # Compile the model with Categorical Cross-Entropy loss and Adam optimizer
  tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=initial_learning_rate, weight_decay=5e-4), metrics=['accuracy','mse'])

  # Display model summary
  tl_model.summary()

  return tl_model

tl_model = build_model(input_shape, mobile)

# Train the model
tl_history = tl_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Save the best model
tl_model.save('convnet_small')
#del tl_model

# Plot the re-trained and the transfer learning training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = tl_model.predict(X_test*255, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## ConvNet X base"""

mobile = tf.keras.applications.ConvNeXtBase(
    model_name="convnext_base",
    include_top=False,
    include_preprocessing=True,
    weights="imagenet",
    input_tensor=None,
    input_shape=input_shape,
    pooling=None,
    classes=2,
    classifier_activation="softmax",
)

tfk.utils.plot_model(mobile, show_shapes=True)

def build_model(input_shape=input_shape, mobile=mobile):

  # Use the supernet as feature extractor, i.e. freeze all its weigths
  mobile.trainable = False

  # Create an input layer with shape (224, 224, 3)
  inputs = tfk.Input(shape=input_shape)

  preprocessing = tfk.Sequential([
      tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
      #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
      #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
      tfkl.RandomRotation(0.2),
      #tfkl.RandomZoom(0.2),
      #tfkl.RandomContrast(0.75),
  ], name='Preprocessing')

  x = preprocessing(inputs)

  # Connect MobileNetV2 to the input
  x = mobile(x)

  x = tf.keras.layers.GlobalAveragePooling2D()(x)

  # Classifier Dense Network
  x = tfkl.Dense(512, activation='relu')(x)
  x = tfkl.Dropout(0.5)(x)
  outputs = tfkl.Dense(2, activation='softmax')(x)

  # Create a Model connecting input and output
  tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

  # Compile the model with Categorical Cross-Entropy loss and Adam optimizer
  tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=initial_learning_rate, weight_decay=5e-4), metrics=['accuracy','mse'])

  # Display model summary
  tl_model.summary()

  return tl_model

tl_model = build_model(input_shape, mobile)

# Train the model
tl_history = tl_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Save the best model
tl_model.save('convnet_base')
#del tl_model

# Plot the re-trained and the transfer learning training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = tl_model.predict(X_test*255, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## ConvNet X large"""

mobile = tf.keras.applications.ConvNeXtLarge(
    model_name="convnext_large",
    include_top=False,
    include_preprocessing=True,
    weights="imagenet",
    input_tensor=None,
    input_shape=input_shape,
    pooling=None,
    classes=2,
    classifier_activation="softmax",
)

tfk.utils.plot_model(mobile, show_shapes=True)

def build_model(input_shape=input_shape, mobile=mobile):

  # Use the supernet as feature extractor, i.e. freeze all its weigths
  mobile.trainable = False

  # Create an input layer with shape (224, 224, 3)
  inputs = tfk.Input(shape=input_shape)

  preprocessing = tfk.Sequential([
      tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
      #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
      #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
      tfkl.RandomRotation(0.2),
      #tfkl.RandomZoom(0.2),
      #tfkl.RandomContrast(0.75),
  ], name='Preprocessing')

  x = preprocessing(inputs)

  # Connect MobileNetV2 to the input
  x = mobile(x)

  x = tf.keras.layers.GlobalAveragePooling2D()(x)

  # Classifier Dense Network
  x = tfkl.Dense(512, activation='relu')(x)
  x = tfkl.Dropout(0.5)(x)
  outputs = tfkl.Dense(2, activation='softmax')(x)

  # Create a Model connecting input and output
  tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

  # Compile the model with Categorical Cross-Entropy loss and Adam optimizer
  tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=initial_learning_rate, weight_decay=5e-4), metrics=['accuracy','mse'])

  # Display model summary
  tl_model.summary()

  return tl_model

tl_model = build_model(input_shape, mobile)

# Train the model
tl_history = tl_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Save the best model
tl_model.save('convnet_large')
#del tl_model

# Plot the re-trained and the transfer learning training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = tl_model.predict(X_test*255, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## ConvNet X Xlarge"""

mobile = tf.keras.applications.ConvNeXtXLarge(
    model_name="convnext_xlarge",
    include_top=False,
    include_preprocessing=True,
    weights="imagenet",
    input_tensor=None,
    input_shape=input_shape,
    pooling=None,
    classes=2,
    classifier_activation="softmax",
)

tfk.utils.plot_model(mobile, show_shapes=True)

def build_model(input_shape=input_shape, mobile=mobile):

  # Use the supernet as feature extractor, i.e. freeze all its weigths
  mobile.trainable = False

  # Create an input layer with shape (224, 224, 3)
  inputs = tfk.Input(shape=input_shape)

  preprocessing = tfk.Sequential([
      tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
      #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
      #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
      tfkl.RandomRotation(0.2),
      #tfkl.RandomZoom(0.2),
      #tfkl.RandomContrast(0.75),
  ], name='Preprocessing')

  x = preprocessing(inputs)

  # Connect MobileNetV2 to the input
  x = mobile(x)

  x = tf.keras.layers.GlobalAveragePooling2D()(x)

  # Classifier Dense Network
  x = tfkl.Dense(512, activation='relu')(x)
  x = tfkl.Dropout(0.5)(x)
  outputs = tfkl.Dense(2, activation='softmax')(x)

  # Create a Model connecting input and output
  tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

  # Compile the model with Categorical Cross-Entropy loss and Adam optimizer
  tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=initial_learning_rate, weight_decay=5e-4), metrics=['accuracy','mse'])

  # Display model summary
  tl_model.summary()

  return tl_model

tl_model = build_model(input_shape, mobile)

# Train the model
tl_history = tl_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 16,
    epochs = 200,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True), lr_scheduler]
).history

# Save the best model
tl_model.save('convnet_Xlarge')
#del tl_model

# Plot the re-trained and the transfer learning training histories
plt.figure(figsize=(15,5))
plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Categorical Crossentropy')
plt.grid(alpha=.3)

plt.figure(figsize=(15,5))
plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')
plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')
plt.legend(loc='upper left')
plt.title('Accuracy')
plt.grid(alpha=.3)

plt.show()

# Predict labels for the entire test set
predictions = tl_model.predict(X_test*255, verbose=0)

# Display the shape of the predictions
print("Predictions Shape:", predictions.shape)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))
precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""# Ensemble Models

## Ensemble model of tiny and small (try)
"""

# Assuming you have three pre-trained models saved
model1 = tfk.models.load_model('convnet_tiny')
model2 = tfk.models.load_model('convnet_small')

# Define weights based on model accuracy (example values, replace with actual accuracies)
weights = [0.5, 0.5]

# Input layer
# model_input = Input(shape=input_shape, name='input_layer')

# Make predictions using individual models
preds1 = model1.predict(X_test*255)
preds2 = model2.predict(X_test*255)

# Combine predictions with weights
weighted_preds = np.average([preds1, preds2], axis=0, weights=weights)

# Get the class with the maximum weighted prediction for each sample
ensemble_predictions = np.argmax(weighted_preds, axis=-1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), ensemble_predictions)

# Evaluate the ensemble model
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
print("Weighted Ensemble Accuracy:", accuracy)

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
precision = precision_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## Ensemble model of tiny, small, base, large e Xlarge (try)"""

# Assuming you have three pre-trained models saved
model1 = tfk.models.load_model('convnet_tiny')
model2 = tfk.models.load_model('convnet_small')
model3 = tfk.models.load_model('convnet_base')
model4 = tfk.models.load_model('convnet_large')
model5 = tfk.models.load_model('convnet_Xlarge')

# Define weights based on model accuracy (example values, replace with actual accuracies)
weights = [0.8433, 0.8529, 0.8977, 0.8769, 0.8905]

# Make predictions using individual models
preds1 = model1.predict(X_test*255)
preds2 = model2.predict(X_test*255)
preds3 = model3.predict(X_test*255)
preds4 = model4.predict(X_test*255)
preds5 = model5.predict(X_test*255)

# Combine predictions with weights
weighted_preds = np.average([preds1, preds2, preds3, preds4, preds5], axis=0, weights=weights)

# Get the class with the maximum weighted prediction for each sample
ensemble_predictions = tf.argmax(weighted_preds, axis=-1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), ensemble_predictions)

# Evaluate the ensemble model
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
print("Weighted Ensemble Accuracy:", accuracy)

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
precision = precision_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## Ensemble model of base convnet and efficientnet"""

# Assuming you have three pre-trained models saved
model1 = tfk.models.load_model('convnet_base')
model2 = tfk.models.load_model('efficientnetv2_with_dropout_lr_scheduling')

# Define weights based on model accuracy (example values, replace with actual accuracies)
weights = [0.8977, 0.8537]

# Make predictions using individual models
preds1 = model1.predict(X_test*255)
preds2 = model2.predict(X_test*255)

# Combine predictions with weights
weighted_preds = np.average([preds1, preds2], axis=0, weights=weights)

# Get the class with the maximum weighted prediction for each sample
ensemble_predictions = tf.argmax(weighted_preds, axis=-1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), ensemble_predictions)

# Evaluate the ensemble model
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
print("Weighted Ensemble Accuracy:", accuracy)

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
precision = precision_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""## Ensemble model of convnets and efficientnet with improved voting system"""

# Assuming you have three pre-trained models saved
model1 = tfk.models.load_model('convnet_tiny')
model2 = tfk.models.load_model('convnet_small')
model3 = tfk.models.load_model('convnet_base')
model4 = tfk.models.load_model('convnet_large')
model5 = tfk.models.load_model('convnet_Xlarge')
model6 = tfk.models.load_model('efficientnetv2_with_dropout_lr_scheduling')

# Define weights based on model accuracy (example values, replace with actual accuracies)
weights = [0.8433, 0.8529, 0.8977, 0.8769, 0.8905, 0.8537]

# Make predictions using individual models
preds1 = model1.predict(X_test*255)
preds2 = model2.predict(X_test*255)
preds3 = model3.predict(X_test*255)
preds4 = model4.predict(X_test*255)
preds5 = model5.predict(X_test*255)
preds6 = model6.predict(X_test*255)

# Combine predictions with weights
weighted_preds = np.average([preds1, preds2, preds3, preds4, preds5, preds6], axis=0, weights=weights)

# Get the class with the maximum weighted prediction for each sample
ensemble_predictions = tf.argmax(weighted_preds, axis=-1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), ensemble_predictions)

# Evaluate the ensemble model
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
print("Weighted Ensemble Accuracy:", accuracy)

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
precision = precision_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""### Change of Voting System

I want to try a different approach now. For every prediction I assign a weight based on the uncertainty of the prediction. For example if I see [0.9, 0.1] I will give a high score, otherwise if I see like [0.4, 0.6] I will give a low score. To do this I will first compute the difference between the elements of the vector of predictions, like $(0.9-0.1)^2$ or $(0.6-0.4)^2$ and use it as weight for the decisions. I feel like I'm going to need to add a little bias to avoid numerical instabilities and that I'll need to add a scale parameter. The end function will be like $P = \alpha * (p[0]-p[1])^2 + \beta$ where $\alpha = 0.2$ and $\beta = 0.01$.

This way I am giving more importance to the units that the classifiers are sure that are in one class with respect to the ones that they are unsure. This might cause problems.
"""

print(preds1[0])
print((preds1[0][0] - preds1[0][1])**2)

print(preds1[-3])
print((preds1[-3][0] - preds1[-3][1])**2)

"""If I have $n$ models, I will obtain $n$ vectors of weights $w_{i=1,...,n}$ for each prediction, $w_i \in \mathcal{R}^m$, where $m$ is the number of predictions. These weights now are multiplied by the predictions to get the votes $v_i = p_i w_i$. The votes of every model are averaged together with the same weight (this might be changed) to get the final class predictions."""

preds1

preds1[:,0]

np.array([w1,w1]).shape

# Assuming you have three pre-trained models saved
#model1 = tfk.models.load_model('convnet_tiny')
#model2 = tfk.models.load_model('convnet_small')
#model3 = tfk.models.load_model('convnet_base')
#model4 = tfk.models.load_model('convnet_large')
#model5 = tfk.models.load_model('convnet_Xlarge')
#model6 = tfk.models.load_model('efficientnetv2_with_dropout_lr_scheduling')

# Define weights based on model accuracy (example values, replace with actual accuracies)
# weights = [0.8433, 0.8529, 0.8977, 0.8769, 0.8905, 0.8537]
weights = [1,1,1,1,1,1]

# Make predictions using individual models
#preds1 = model1.predict(X_test*255)
#preds2 = model2.predict(X_test*255)
#preds3 = model3.predict(X_test*255)
#preds4 = model4.predict(X_test*255)
#preds5 = model5.predict(X_test*255)
#preds6 = model6.predict(X_test*255)

def compute_weights(predictions):
    weights = (predictions[:,0] - predictions[:,1])**2
    return np.array([weights, weights]).T

w1 = compute_weights(preds1)
w2 = compute_weights(preds2)
w3 = compute_weights(preds3)
w4 = compute_weights(preds4)
w5 = compute_weights(preds5)
w6 = compute_weights(preds6)

p1 = np.multiply(preds1,w1)
p2 = np.multiply(preds2,w2)
p3 = np.multiply(preds3,w3)
p4 = np.multiply(preds4,w4)
p5 = np.multiply(preds5,w5)
p6 = np.multiply(preds6,w6)

# Combine predictions with weights
weighted_preds = np.average([p1,p2,p3,p4,p5,p6], axis=0, weights=weights)

# Get the class with the maximum weighted prediction for each sample
ensemble_predictions = tf.argmax(weighted_preds, axis=-1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1), ensemble_predictions)

# Evaluate the ensemble model
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
print("Weighted Ensemble Accuracy:", accuracy)

# Compute classification metrics
accuracy = accuracy_score(np.argmax(y_test, axis=-1), ensemble_predictions)
precision = precision_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
recall = recall_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')
f1 = f1_score(np.argmax(y_test, axis=-1), ensemble_predictions, average='macro')

# Display the computed metrics
print('Accuracy:', accuracy.round(4))
print('Precision:', precision.round(4))
print('Recall:', recall.round(4))
print('F1:', f1.round(4))

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)
plt.xlabel('True labels')
plt.ylabel('Predicted labels')
plt.show()

"""I have found that $\alpha$ and $\beta$ are useless. I tried to compute the power of 4 and the result gets slightly better: $P = (p[0]-p[1])^4$. I think I will keep the power of 2 since it preserves the structure of Hilbert space of the space of th probabilities.

# Siamese Neural Networks

## Drive
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/ANNDL

"""### Importing Libraries"""

# Fix randomness and hide warnings
seed = 42

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=Warning)

import numpy as np
np.random.seed(seed)

import logging

import random
random.seed(seed)

#pip install keras-cv

# Import tensorflow
import tensorflow as tf
from tensorflow import keras as tfk
#import keras_cv
from tensorflow.keras import layers as tfkl
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel(logging.ERROR)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)
print(tf.__version__)

# Import other libraries
import cv2
from tensorflow.keras.applications.mobilenet import preprocess_input
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
import seaborn as sns

import pandas as pd

"""### Load data"""

public_data = np.load('public_data.npz', allow_pickle=True)

data = public_data['data']/255
labels = public_data['labels']

print(data.shape)
print(labels.shape)

"""We have 5200 samples of 96x96 images encoded in RGB."""

c_h = 0
c_u = 0
for label in labels:
  if label=='healthy':
    c_h += 1
  elif label=='unhealthy':
    c_u += 1

print(c_h)
print(c_u)

"""### Visualize Data and Data Cleaning

######Let's visualize a part of the dataset with labels and then let's visualize all the dataset with indexed to find out possile outliers
"""

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(data[i])
  titolo = labels[i]
  ax.set_title(titolo)

"""batch_size = 100  # Adjust as needed
for i in range(0, len(data), batch_size):
    batch = data[i:i + batch_size]

    # Display images in the current batch
    for j, image in enumerate(batch):
        plt.subplot(10, 10, j + 1)  # Adjust subplot parameters as needed
        plt.imshow(image)
        #titolo = labels[i]
        plt.axis('off')
        plt.title(str(i + j))

    plt.show()

####Removing Outliers

By inspecting the whole dataset we noticed that there are only 2 types of outlier images. Let's confront them and remove them.
"""

first_shrek = data[95]
first_tro = data[338]

outliers = list()
outliers_labels = list()
outliers_indices = list()

for i in range(0,data.shape[0]):
  if (data[i] == first_shrek).all() or (data[i] == first_tro).all():
    outliers.append(data[i])
    outliers_labels.append(labels[i])
    outliers_indices.append(i)

num_img = 10
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(10):
  ax = axes[i%2, i%num_img//2]
  ax.imshow(outliers[i])
  ax.set_title(outliers_labels[i])

"""Let's drop the shreks and trollolos from the dataset to obtain a cleaned one"""

data = np.delete(data, outliers_indices, axis = 0)
labels = np.delete(labels, outliers_indices)
data.shape

"""We can now visualize the whole dataset to check if every outlier has been removed

batch_size = 100  # Adjust as needed
for i in range(0, len(data), batch_size):
    batch = data[i:i + batch_size]

    # Display images in the current batch
    for j, image in enumerate(batch):
        plt.subplot(10, 10, j + 1)  # Adjust subplot parameters as needed
        plt.imshow(image)
        #titolo = labels[i]
        plt.axis('off')
        plt.title(str(i + j))

    plt.show()
"""

c_h = 0
c_u = 0
for label in labels:
  if label=='healthy':
    c_h += 1
  elif label=='unhealthy':
    c_u += 1

print(c_h)
print(c_u)

class_weights = [c_h/data.shape[0], c_u/data.shape[0]]
print(class_weights)

class_weights = {0: class_weights[0], 1: class_weights[1]}
print(class_weights)

"""#### Load the Clean Dataset

Let's proceed with saving the cleaned dataset in npz format
"""

np.savez('cleaned_public_data.npz', data=data*255, labels=labels)

"""To load the clean dataset"""

public_data = np.load('cleaned_public_data.npz', allow_pickle=True)
data = public_data['data']/255
labels = public_data['labels']
print(data.shape)
print(labels.shape)

"""### Split

Now we use one-hot encoding to our categorical labels to be able to feed them to the network. Maybe we could avoid one-hot since we onlky have 2 classes and turn this into a regression problem.
"""

# Convert to a pandas Series
labels_series = pd.Series(labels)

# Convert to categorical variable
categorical_labels = labels_series.astype('category')

# Convert to category codes
category_codes = categorical_labels.cat.codes

# Use tfk.utils.to_categorical to one-hot encode the category codes
y = tfk.utils.to_categorical(category_codes, num_classes=len(categorical_labels.cat.categories))

# The result is a one-hot encoded representation of your labels
print("One-Hot Encoded Labels:")
print(y)

"""And we split our dataset into training, validation and test set."""

X = data

# Split data into train_val and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=.25, stratify=np.argmax(y,axis=1))

# Further split train_val into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=np.argmax(y_train_val,axis=1))

# Print shapes of the datasets
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

"""Ok, now we have a training set, a validation set and a test set.

### Build the model: hyperparameters

NB. for the function build_model we have to use the output_shape without expanding dimensions
"""

# Define input shape, output shape, batch size, and number of epochs
input_shape = X_train.shape[1:]
output_shape_not_expanded = y_train.shape[1:]
output_shape = np.expand_dims(output_shape_not_expanded, axis=-1)
batch_size = 32
epochs = 1000

# Print input shape, batch size, and number of epochs
print(f"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}")

initial_learning_rate = 0.01

lr_patience = 5
lr_scheduler = tfk.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',     # Metric to monitor (validation mean squared error in this case)
    patience=lr_patience,  # Number of epochs with no improvement after which learning rate will be reduced
    factor=0.9,          # Factor by which the learning rate will be reduced (0.999 in this case)
    mode='max',            # Mode to decide when to reduce learning rate ('min' means reduce when metric stops decreasing)
    min_lr=1e-5            # Minimum learning rate
)

"""### Preparing data for siamese neural network"""

# Function to create pairs of images for Siamese network
def create_pairs(images, labels):
    pairs = []  # List to store pairs
    pair_labels = []  # List to store pair labels (0 or 1)

    # Indices for each class label
    class_indices = [np.where(labels == i)[0] for i in range(2)]  # 2 classes

    for label in range(2):  # For each class label
        for i in range(len(class_indices[label])-1):
            #print(label)
            # Pair with the same class label
            pairs += [[images[class_indices[label][i]], images[class_indices[label][i + 1]]]]
            pair_labels += [1]  # Similar pair

    for label in range(2):  # For each class label
        for i in range(min(len(class_indices[0]), len(class_indices[1]))):
            # Pair with a different class label
            other_label = (label + 1) % 2
            pairs += [[images[class_indices[label][i]], images[class_indices[other_label][i]]]]
            pair_labels += [0]  # Dissimilar pair

    return np.array(pairs), np.array(pair_labels)

# Create pairs for the Siamese network
pairs, pair_labels = create_pairs(data, category_codes)

plt.imshow(pairs[0][1])
print(pair_labels[0])

"""## SimCLR with ResNet backbone"""

import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import backend as K

# Load a pre-trained ResNet50 model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
base_model.trainable = False  # Set to True if needed

# Projection head for learning representations
projection_head = tf.keras.Sequential([
    layers.GlobalAveragePooling2D(),
    layers.Dense(128),
])

# Create inputs for two augmented versions of the same image
input_a = tf.keras.Input(shape=input_shape)
input_b = tf.keras.Input(shape=input_shape)

# Obtain representations for the two augmented images
encoded_a = projection_head(base_model(input_a))
encoded_b = projection_head(base_model(input_b))

# Normalize the representations
encoded_a = tf.math.l2_normalize(encoded_a, axis=1)
encoded_b = tf.math.l2_normalize(encoded_b, axis=1)

# Define the temperature for the contrastive loss
temperature = 0.1

# Implement contrastive loss
def contrastive_loss(y_true, y_pred):
    # y_true is not used in the loss calculation
    # y_pred contains the normalized representations

    # Gather positive and negative pairs
    positive_sim = tf.reduce_sum(tf.multiply(y_pred[0], y_pred[1]), axis=1)
    batch_size = tf.shape(y_pred[0])[0]

    # Calculate pairwise similarity matrix
    pairwise_sim = tf.matmul(y_pred, y_pred, transpose_b=True) / temperature

    # Fill the diagonal with very large negative value to exclude it from the max calculation
    mask = 1 - tf.eye(batch_size)
    pairwise_sim = pairwise_sim - 1e9 * mask

    # Calculate the numerator and denominator of the loss
    exp_sim = tf.exp(pairwise_sim)
    denominator = tf.reduce_sum(exp_sim, axis=1)

    # Calculate the loss
    loss = -tf.math.log(positive_sim / denominator)
    return tf.reduce_mean(loss)

# Create the SimCLR model
simclr_model = Model(inputs=[input_a, input_b], outputs=[encoded_a, encoded_b])

# Compile the model using the contrastive loss
simclr_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=contrastive_loss)

# Train the SimCLR model using your dataset
# Replace this with your own data and training configuration
# Example code for training:
# simclr_model.fit(your_dataset, epochs=num_epochs, batch_size=batch_size)

simclr_model.summary()

tfk.utils.plot_model(simclr_model, show_shapes=True)

# Assuming you have the SimCLR model and data prepared
# simclr_model is the model configured for SimCLR training

# Function for training a single step
@tf.function
def train_step(image_batch_a, image_batch_b):
    with tf.GradientTape() as tape:
        # Forward pass through the SimCLR model
        encoded_a, encoded_b = simclr_model([image_batch_a, image_batch_b])

        # Calculate the contrastive loss
        loss = contrastive_loss(None, [encoded_a, encoded_b])

    # Compute gradients
    gradients = tape.gradient(loss, simclr_model.trainable_variables)

    # Update weights using optimizer
    optimizer.apply_gradients(zip(gradients, simclr_model.trainable_variables))

    return loss

# Training loop
num_epochs = 10
batch_size = 32
optimizer = tf.keras.optimizers.Adam()

for epoch in range(num_epochs):
    epoch_loss = 0.0
    num_batches = 0

    # Iterate over your dataset batches
    for image_batch_a, image_batch_b in your_dataset.batch(batch_size):
        loss = train_step(image_batch_a, image_batch_b)
        epoch_loss += loss
        num_batches += 1

    # Calculate average epoch loss
    average_epoch_loss = epoch_loss / num_batches

    # Display or log the average loss for this epoch
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {average_epoch_loss:.4f}")

# Train the SimCLR model using the fit function
simclr_model.fit(your_dataset, epochs=num_epochs, batch_size=batch_size)

"""## Siamese NN from github"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install git+https://github.com/aspamers/siamese

from siamese import SiameseNetwork

def create_base_model(input_shape):
    base_model = tfk.applications.ResNet50(
        input_shape=input_shape,
        include_top=False,
        weights="imagenet",
        pooling='avg',
        classes=2
    )

    base_model.trainable = False  # Set to True if needed

    inputs = tfk.Input(shape=input_shape)

    x = inputs
    x = base_model(x)
    x = layers.Dense(128)(x)

    return Model(x)

base_model = tfk.applications.EfficientNetV2S(
    input_shape=input_shape,
    include_top=False,
    weights="imagenet",
    pooling='avg',
    include_preprocessing=True,
)

base_model.trainable = False  # Set to True if needed


# =============================
# TRAINABLE WEIGHTS

# Set all layers as trainable
base_model.trainable = True
for i, layer in enumerate(base_model.layers):
   print(i, layer.name, layer.trainable)

# Freeze first N layers, e.g., until the 133rd one
N = 100
for i, layer in enumerate(base_model.layers[:N]):
  layer.trainable=False
for i, layer in enumerate(base_model.layers):
   print(i, layer.name, layer.trainable)

# =============================



inputs = tfk.Input(shape=input_shape)


preprocessing = tfk.Sequential([
     tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),
     #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),
     #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop'),
     tfkl.RandomRotation(0.2),
     #tfkl.RandomZoom(0.2),
     #tfkl.RandomContrast(0.75),
], name='Preprocessing')

x = preprocessing(inputs)
x = base_model(x)
outputs = tfkl.Dense(128)(x)

# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model1')
tl_model.summary()

base_model = tl_model

def create_head_model(embedding_shape):
    embedding_a = tfk.Input(shape=embedding_shape)
    embedding_b = tfk.Input(shape=embedding_shape)

    head = tfkl.Concatenate()([embedding_a, embedding_b])
    head = tfkl.Dense(4)(head)
    head = tfkl.BatchNormalization()(head)
    head = tfkl.Activation(activation='sigmoid')(head)

    head = tfkl.Dense(1)(head)
    head = tfkl.BatchNormalization()(head)
    head = tfkl.Activation(activation='sigmoid')(head)

    return tfk.Model([embedding_a, embedding_b], head)

embedding_shape = base_model.output_shape

embedding_a = tfk.Input(shape=embedding_shape)
embedding_b = tfk.Input(shape=embedding_shape)

head = tfkl.Concatenate()([embedding_a, embedding_b])
head = tfkl.Dense(64)(head)
#head = tfkl.BatchNormalization()(head)
head = tfkl.Activation(activation='relu')(head)
head = tfkl.Dense(32)(head)
#head = tfkl.BatchNormalization()(head)
head = tfkl.Activation(activation='relu')(head)
head = tfkl.Dense(32)(head)
#head = tfkl.BatchNormalization()(head)
head = tfkl.Activation(activation='relu')(head)
head = tfkl.Dense(1)(head)
#head = tfkl.BatchNormalization()(head)
head = tfkl.Activation(activation='relu')(head)

head_model = tfk.Model([embedding_a, embedding_b], head)

head_model.summary()

siamese_network = SiameseNetwork(base_model, head_model)
siamese_network.compile(loss='binary_crossentropy', optimizer=tfk.optimizers.AdamW(),metrics=['accuracy'])
siamese_checkpoint_path = "./siamese_checkpoint"
siamese_callbacks = [
    tfk.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=0, restore_best_weights=True),
    tfk.callbacks.ModelCheckpoint(siamese_checkpoint_path, monitor='val_acc', save_best_only=True, verbose=0)
]

"""this library needs y encoded as 0,1 not one-hot."""

X = data
y = category_codes
# Split data into train_val and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=.25, stratify=y)

# Further split train_val into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=y_train_val)

# Print shapes of the datasets
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

plt.imshow(data[0])

siamese_network.fit(
    X_train*255,
    y_train,
    validation_data=(X_val*255, y_val),
    batch_size=64,
    epochs=epochs,
    callbacks=siamese_callbacks
)